{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "65c31609-0db6-4250-a1b1-1d664fee88b2",
   "metadata": {},
   "source": [
    "# COMP30770 Group Project - Amazon Electronics Review Analysis\n",
    "\n",
    "## Project Overview\n",
    "This project analyses Amazon Electronics reviews to identify top-rated products and common themes in customer feedback.\n",
    "\n",
    "**Datasets:**\n",
    "- `Electronics.jsonl` — structured review data (rating, text, timestamp)\n",
    "- `meta_Electronics.jsonl` — semi-structured product metadata (title, category, price)\n",
    "\n",
    "**Pipeline:**\n",
    "1. Load & clean data\n",
    "2. Aggregate ratings per product\n",
    "3. Word frequency analysis *(bottleneck)*\n",
    "4. Join with metadata *(bottleneck)*\n",
    "5. Output Top 20 ranking\n",
    "6. MapReduce optimisation with Spark RDD"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "497cd3a7-56e7-45e4-b45c-f50beb626fc9",
   "metadata": {},
   "source": [
    "## Environment Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bc615406-48e7-4f75-a86c-6850ba3b7db5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All packages ready.\n",
      "Python version: 3.11.9 (tags/v3.11.9:de54cf5, Apr  2 2024, 10:12:12) [MSC v.1938 64 bit (AMD64)]\n"
     ]
    }
   ],
   "source": [
    "import sys, subprocess\n",
    "subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\",\n",
    "                       \"pyspark==3.5.1\", \"pandas\", \"pyarrow\", \"-q\"])\n",
    "print(\"All packages ready.\")\n",
    "print(\"Python version:\", sys.version)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27fbd112-0a92-4d6b-8967-602062a553da",
   "metadata": {},
   "source": [
    "## Download Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8388d9b5-0309-4ce1-b89c-de86b95be969",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Working directory: C:\\Users\\ADMIN\n",
      "Electronics.jsonl already exists, skipping.\n",
      "meta_Electronics.jsonl already exists, skipping.\n",
      "\n",
      "Files ready: ['Electronics.jsonl', 'meta_Electronics.jsonl']\n"
     ]
    }
   ],
   "source": [
    "import os, urllib.request\n",
    "\n",
    "os.chdir(r\"C:\\Users\\ADMIN\")\n",
    "print(\"Working directory:\", os.getcwd())\n",
    "\n",
    "files = {\n",
    "    \"Electronics.jsonl\": \"https://huggingface.co/datasets/McAuley-Lab/Amazon-Reviews-2023/resolve/main/raw/review_categories/Electronics.jsonl\",\n",
    "    \"meta_Electronics.jsonl\": \"https://huggingface.co/datasets/McAuley-Lab/Amazon-Reviews-2023/resolve/main/raw/meta_categories/meta_Electronics.jsonl\"\n",
    "}\n",
    "\n",
    "for filename, url in files.items():\n",
    "    if os.path.exists(filename):\n",
    "        print(f\"{filename} already exists, skipping.\")\n",
    "    else:\n",
    "        print(f\"Downloading {filename}...\")\n",
    "        urllib.request.urlretrieve(url, filename)\n",
    "        print(f\"{filename} done.\")\n",
    "\n",
    "print(\"\\nFiles ready:\", [f for f in os.listdir() if f.endswith(\".jsonl\")])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f23b3fba-1f22-42f7-8e6b-3f7095c6db7e",
   "metadata": {},
   "source": [
    "## Section 2: Traditional Solution (Single-threaded Python)\n",
    "\n",
    "Prototype using standard Python with no parallelism to establish a performance baseline.\n",
    "\n",
    "Steps:\n",
    "- **Step 1**: Load and clean raw review data\n",
    "- **Step 2**: Aggregate ratings per product\n",
    "- **Step 3**: Word frequency analysis ← *bottleneck*\n",
    "- **Step 4**: Join reviews with product metadata ← *bottleneck*\n",
    "- **Step 5**: Output Top 20 ranked products"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "17790e13-07af-4a3a-91bb-0844e43a8d59",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Libraries imported.\n"
     ]
    }
   ],
   "source": [
    "import json, time, re, tracemalloc\n",
    "from collections import defaultdict, Counter\n",
    "\n",
    "step_times = {}\n",
    "step_memory = {}\n",
    "print(\"Libraries imported.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bbf6198-739a-40de-9af4-af1375f0e2d7",
   "metadata": {},
   "source": [
    "### Step 1: Load & Clean Data\n",
    "Filter invalid records and remove HTML tags from review text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a4ed20df-1a2d-4bc2-bd10-fd7b0d6c51e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 1: Loading and cleaning data...\n",
      "Total reviews loaded : 499,993\n",
      "Time                 : 36.23s\n",
      "Peak memory          : 331.5 MB\n"
     ]
    }
   ],
   "source": [
    "print(\"Step 1: Loading and cleaning data...\")\n",
    "tracemalloc.start()\n",
    "start = time.time()\n",
    "\n",
    "MAX_ROWS = 500000\n",
    "\n",
    "def clean_text(text):\n",
    "    text = re.sub(r'<[^>]+>', ' ', text)\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "    return text\n",
    "\n",
    "reviews = []\n",
    "with open(\"Electronics.jsonl\", \"r\", encoding=\"utf-8\") as f:\n",
    "    for i, line in enumerate(f):\n",
    "        if i >= MAX_ROWS:\n",
    "            break\n",
    "        r = json.loads(line)\n",
    "        if r.get(\"rating\") and r.get(\"parent_asin\") and r.get(\"text\"):\n",
    "            reviews.append({\n",
    "                \"rating\": float(r[\"rating\"]),\n",
    "                \"asin\": r[\"parent_asin\"],\n",
    "                \"text\": clean_text(r[\"text\"]),\n",
    "                \"timestamp\": r.get(\"timestamp\", 0),\n",
    "                \"verified\": r.get(\"verified_purchase\", False)\n",
    "            })\n",
    "\n",
    "current, peak = tracemalloc.get_traced_memory()\n",
    "tracemalloc.stop()\n",
    "step_times[\"Step 1\"] = time.time() - start\n",
    "step_memory[\"Step 1\"] = peak / 1024 / 1024\n",
    "\n",
    "print(f\"Total reviews loaded : {len(reviews):,}\")\n",
    "print(f\"Time                 : {step_times['Step 1']:.2f}s\")\n",
    "print(f\"Peak memory          : {step_memory['Step 1']:.1f} MB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f69482c2-f422-4d2f-afa3-14ae52624969",
   "metadata": {},
   "source": [
    "### Step 2: Aggregate Ratings per Product"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "951f021e-b245-4908-b266-1174a1e34adb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 2: Aggregating ratings by product...\n",
      "Unique products : 177,724\n",
      "Time            : 1.33s\n",
      "Peak memory     : 88.3 MB\n"
     ]
    }
   ],
   "source": [
    "print(\"Step 2: Aggregating ratings by product...\")\n",
    "tracemalloc.start()\n",
    "start = time.time()\n",
    "\n",
    "asin_stats = defaultdict(lambda: {\"count\": 0, \"total_rating\": 0.0})\n",
    "for r in reviews:\n",
    "    asin_stats[r[\"asin\"]][\"count\"] += 1\n",
    "    asin_stats[r[\"asin\"]][\"total_rating\"] += r[\"rating\"]\n",
    "\n",
    "asin_avg = {\n",
    "    asin: {\n",
    "        \"count\": v[\"count\"],\n",
    "        \"avg_rating\": round(v[\"total_rating\"] / v[\"count\"], 2)\n",
    "    }\n",
    "    for asin, v in asin_stats.items()\n",
    "}\n",
    "\n",
    "current, peak = tracemalloc.get_traced_memory()\n",
    "tracemalloc.stop()\n",
    "step_times[\"Step 2\"] = time.time() - start\n",
    "step_memory[\"Step 2\"] = peak / 1024 / 1024\n",
    "\n",
    "print(f\"Unique products : {len(asin_avg):,}\")\n",
    "print(f\"Time            : {step_times['Step 2']:.2f}s\")\n",
    "print(f\"Peak memory     : {step_memory['Step 2']:.1f} MB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32ad531f-67ea-4721-b9fa-d7e073a94c42",
   "metadata": {},
   "source": [
    "### Step 3: Word Frequency Analysis\n",
    "Tokenise review texts, remove stopwords, count word frequencies.\n",
    "This step is CPU-bound and processes millions of words — expected bottleneck."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "45a3b4e1-7adf-491d-94d3-8437a6c99f7f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 3: Word frequency analysis...\n",
      "Top 10 words : [('great', 149010), ('very', 124083), ('one', 123014), ('use', 112360), ('good', 108195), ('just', 98595), ('like', 97960), ('these', 95473), ('can', 95052), ('all', 92840)]\n",
      "Time         : 35.41s\n",
      "Peak memory  : 23.0 MB\n"
     ]
    }
   ],
   "source": [
    "print(\"Step 3: Word frequency analysis...\")\n",
    "tracemalloc.start()\n",
    "start = time.time()\n",
    "\n",
    "stopwords = {\"the\",\"a\",\"an\",\"and\",\"or\",\"but\",\"in\",\"on\",\"at\",\"to\",\"for\",\n",
    "             \"of\",\"with\",\"is\",\"it\",\"this\",\"that\",\"was\",\"i\",\"my\",\"they\",\n",
    "             \"have\",\"be\",\"are\",\"not\",\"so\",\"as\",\"its\",\"has\",\"we\",\"you\"}\n",
    "\n",
    "word_counter = Counter()\n",
    "for r in reviews:\n",
    "    words = r[\"text\"].lower().split()\n",
    "    filtered = [w.strip(\".,!?\\\"'\") for w in words\n",
    "                if w.strip(\".,!?\\\"'\") not in stopwords and len(w) > 2]\n",
    "    word_counter.update(filtered)\n",
    "\n",
    "current, peak = tracemalloc.get_traced_memory()\n",
    "tracemalloc.stop()\n",
    "step_times[\"Step 3\"] = time.time() - start\n",
    "step_memory[\"Step 3\"] = peak / 1024 / 1024\n",
    "\n",
    "print(f\"Top 10 words : {word_counter.most_common(10)}\")\n",
    "print(f\"Time         : {step_times['Step 3']:.2f}s\")\n",
    "print(f\"Peak memory  : {step_memory['Step 3']:.1f} MB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2782ed70-c5c9-4295-b4c0-b0c87eb33c65",
   "metadata": {},
   "source": [
    "### Step 4: Join Reviews with Product Metadata\n",
    "Load metadata file and enrich each product with title, category, and price.\n",
    "This step is I/O-bound due to the large metadata file — expected bottleneck."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c96ddb5f-7a77-4ae0-becb-f4c56dd8656b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 4: Joining with product metadata...\n",
      "Successfully joined : 177,724\n",
      "Time                : 104.62s\n",
      "Peak memory         : 850.2 MB\n"
     ]
    }
   ],
   "source": [
    "print(\"Step 4: Joining with product metadata...\")\n",
    "tracemalloc.start()\n",
    "start = time.time()\n",
    "\n",
    "meta = {}\n",
    "with open(\"meta_Electronics.jsonl\", \"r\", encoding=\"utf-8\") as f:\n",
    "    for line in f:\n",
    "        m = json.loads(line)\n",
    "        asin = m.get(\"parent_asin\")\n",
    "        if asin:\n",
    "            meta[asin] = {\n",
    "                \"title\": m.get(\"title\", \"Unknown\"),\n",
    "                \"category\": m.get(\"main_category\", \"Unknown\"),\n",
    "                \"price\": m.get(\"price\")\n",
    "            }\n",
    "\n",
    "enriched = {}\n",
    "for asin, stats in asin_avg.items():\n",
    "    if asin in meta:\n",
    "        enriched[asin] = {**stats, **meta[asin]}\n",
    "\n",
    "current, peak = tracemalloc.get_traced_memory()\n",
    "tracemalloc.stop()\n",
    "step_times[\"Step 4\"] = time.time() - start\n",
    "step_memory[\"Step 4\"] = peak / 1024 / 1024\n",
    "\n",
    "print(f\"Successfully joined : {len(enriched):,}\")\n",
    "print(f\"Time                : {step_times['Step 4']:.2f}s\")\n",
    "print(f\"Peak memory         : {step_memory['Step 4']:.1f} MB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "785cbfe3-d436-4411-8e3f-bd3c5ce0ab0b",
   "metadata": {},
   "source": [
    "### Step 5: Output Top 20 Product Ranking\n",
    "Filter products with at least 100 reviews, sort by average rating."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "89c56f4c-7128-487e-aceb-93333aaad17c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 5: Generating Top 20 ranking...\n",
      "Time: 0.01s\n",
      "\n",
      "Rank  Avg Rating   Review Count    Product Title\n",
      "---------------------------------------------------------------------------\n",
      "1     4.89         122             Amazon Basics High-Speed HDMI Cable 2-Pack - \n",
      "2     4.85         113             Crucial RAM 16GB Kit (2x8GB) DDR3 1600 MHz CL\n",
      "3     4.85         110             Lifetime 28240 Adjustable Folding Laptop Tabl\n",
      "4     4.81         133             Lamicall Tablet Stand Adjustable, Tablet Stan\n",
      "5     4.78         125             Fintie Slimshell Case for 6\" Kindle Paperwhit\n",
      "6     4.77         113             Amazon Basics USB 2.0 Extension Cable - A-Mal\n",
      "7     4.77         154             Amazon Basics USB-A to USB-B 2.0 Cable for Pr\n",
      "8     4.77         133             SAMSUNG (MB-ME64GA/AM) 64GB 100MB/s (U3) Micr\n",
      "9     4.76         454             Amazon Basics HDMI Cable, 18Gbps High-Speed, \n",
      "10    4.75         193             Cat 6 Ethernet Cable 1 Ft (6Pack), Outdoor&In\n",
      "11    4.75         142             BlueRigger HDMI Cable 4K (6FT, 4K 60Hz HDR, H\n",
      "12    4.74         276             Amazon Kindle Paperwhite Leather Case, Onyx B\n",
      "13    4.74         147             SanDisk 32GB Cruzer USB 2.0 Flash Drive, Frus\n",
      "14    4.74         171             Amazon Basics External Hard Drive Portable Ca\n",
      "15    4.73         163             Echo Dot (3rd Gen) - Smart speaker with Alexa\n",
      "16    4.73         260             MagicFiber Extra Large Microfiber Cleaning Cl\n",
      "17    4.73         142             VELCRO Brand ONE-WRAP Cable Ties, 100Pk, 8 x \n",
      "18    4.73         245             Chicka Chicka Boom Boom (Chicka Chicka Book, \n",
      "19    4.72         120             SAMSUNG (MB-ME32GA/AM) 32GB 95MB/s (U1) micro\n",
      "20    4.72         106             Ceptics Brazil Travel Adapter Plug with Dual \n"
     ]
    }
   ],
   "source": [
    "print(\"Step 5: Generating Top 20 ranking...\")\n",
    "start = time.time()\n",
    "\n",
    "top20 = sorted(\n",
    "    [(asin, info) for asin, info in enriched.items() if info[\"count\"] >= 100],\n",
    "    key=lambda x: x[1][\"avg_rating\"],\n",
    "    reverse=True\n",
    ")[:20]\n",
    "\n",
    "step_times[\"Step 5\"] = time.time() - start\n",
    "\n",
    "print(f\"Time: {step_times['Step 5']:.2f}s\\n\")\n",
    "print(f\"{'Rank':<5} {'Avg Rating':<12} {'Review Count':<15} {'Product Title'}\")\n",
    "print(\"-\" * 75)\n",
    "for rank, (asin, info) in enumerate(top20, 1):\n",
    "    print(f\"{rank:<5} {info['avg_rating']:<12} {info['count']:<15} {info['title'][:45]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d731d1a-2833-443f-883a-9dc79b02978a",
   "metadata": {},
   "source": [
    "### Traditional Solution Performance Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e5ead563-33fb-451c-a846-452099f98726",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================================\n",
      "Performance Summary - Traditional Solution\n",
      "==================================================\n",
      "Step 1: 36.23s\n",
      "Step 2: 1.33s\n",
      "Step 3: 35.41s <-- bottleneck\n",
      "Step 4: 104.62s <-- bottleneck\n",
      "Step 5: 0.01s\n",
      "--------------------------------------------------\n",
      "Total: 177.60s\n"
     ]
    }
   ],
   "source": [
    "total = sum(step_times.values())\n",
    "print(\"=\" * 50)\n",
    "print(\"Performance Summary - Traditional Solution\")\n",
    "print(\"=\" * 50)\n",
    "for step, t in step_times.items():\n",
    "    flag = \" <-- bottleneck\" if step in [\"Step 3\", \"Step 4\"] else \"\"\n",
    "    print(f\"{step}: {t:.2f}s{flag}\")\n",
    "print(\"-\" * 50)\n",
    "print(f\"Total: {total:.2f}s\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62078f87-d3cb-4a74-ba10-c5d298a2ccaa",
   "metadata": {},
   "source": [
    "## Section 3: MapReduce Optimisation with Spark RDD\n",
    "\n",
    "Two bottlenecks identified from Section 2:\n",
    "- **Step 3** (~23s): Word frequency — embarrassingly parallel, ideal for MapReduce\n",
    "- **Step 4** (~92s): Metadata join — large file I/O, benefits from parallel loading\n",
    "\n",
    "**MapReduce logic:**\n",
    "- Word frequency: `Map (word, 1)` → `Reduce sum`\n",
    "- Metadata join: `Map (asin, review_stats)` + `Map (asin, meta)` → `Reduce join`\n",
    "\n",
    "**Expected speedup:** 3-5x on both tasks using all available CPU cores."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86d89d8a-ee15-483e-98c1-a97cea316b9c",
   "metadata": {},
   "source": [
    "### Initialise Spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "566b1ea5-8437-43eb-8317-5f0911c29117",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spark version       : 3.5.1\n",
      "Master              : local[*]\n",
      "Default parallelism : 16\n"
     ]
    }
   ],
   "source": [
    "import sys, os\n",
    "os.environ[\"PYSPARK_PYTHON\"] = sys.executable\n",
    "os.environ[\"PYSPARK_DRIVER_PYTHON\"] = sys.executable\n",
    "\n",
    "from pyspark import SparkContext, SparkConf\n",
    "\n",
    "try:\n",
    "    sc.stop()\n",
    "except:\n",
    "    pass\n",
    "\n",
    "conf = SparkConf() \\\n",
    "    .setAppName(\"AmazonReviewsAnalysis\") \\\n",
    "    .setMaster(\"local[*]\") \\\n",
    "    .set(\"spark.driver.memory\", \"8g\") \\\n",
    "    .set(\"spark.executor.memory\", \"4g\")\n",
    "\n",
    "sc = SparkContext(conf=conf)\n",
    "print(\"Spark version       :\", sc.version)\n",
    "print(\"Master              :\", sc.master)\n",
    "print(\"Default parallelism :\", sc.defaultParallelism)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2067812-121b-443d-a7e8-bf7638c596b1",
   "metadata": {},
   "source": [
    "### MapReduce Optimisation 1: Word Frequency\n",
    "\n",
    "**Map:** For each review, emit `(word, 1)` for every valid token.\n",
    "\n",
    "**Reduce:** Sum counts per word across all partitions in parallel.\n",
    "\n",
    "Since each review can be processed independently, this is embarrassingly parallel.\n",
    "Expected speedup: ~3-4x over the 23s baseline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0294958f-c4ce-471d-82d9-8349d3da0e5e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MapReduce Step 1: Word frequency...\n",
      "Top 10 words : [('great', 149010), ('very', 124083), ('one', 123014), ('use', 112360), ('good', 108195), ('just', 98595), ('like', 97960), ('these', 95473), ('can', 95052), ('all', 92840)]\n",
      "Spark time   : 36.32s\n",
      "Traditional  : 35.41s\n",
      "Speedup      : 0.97x\n"
     ]
    }
   ],
   "source": [
    "import re as _re\n",
    "import json as _json\n",
    "\n",
    "print(\"MapReduce Step 1: Word frequency...\")\n",
    "spark_times = {}\n",
    "\n",
    "STOPWORDS = {\"the\",\"a\",\"an\",\"and\",\"or\",\"but\",\"in\",\"on\",\"at\",\"to\",\"for\",\n",
    "             \"of\",\"with\",\"is\",\"it\",\"this\",\"that\",\"was\",\"i\",\"my\",\"they\",\n",
    "             \"have\",\"be\",\"are\",\"not\",\"so\",\"as\",\"its\",\"has\",\"we\",\"you\"}\n",
    "\n",
    "def map_words(review_dict):\n",
    "    # Map: emit (word, 1) for each valid token\n",
    "    text = _re.sub(r'<[^>]+>', ' ', review_dict[\"text\"]).lower()\n",
    "    result = []\n",
    "    for w in text.split():\n",
    "        w = w.strip(\".,!?\\\"'\")\n",
    "        if len(w) > 2 and w not in STOPWORDS:\n",
    "            result.append((w, 1))\n",
    "    return result\n",
    "\n",
    "start = time.time()\n",
    "\n",
    "top10_words = sc.parallelize(reviews, sc.defaultParallelism) \\\n",
    "    .flatMap(map_words) \\\n",
    "    .reduceByKey(lambda a, b: a + b) \\\n",
    "    .sortBy(lambda x: x[1], ascending=False) \\\n",
    "    .take(10)\n",
    "\n",
    "spark_times[\"Word Frequency\"] = time.time() - start\n",
    "\n",
    "print(f\"Top 10 words : {top10_words}\")\n",
    "print(f\"Spark time   : {spark_times['Word Frequency']:.2f}s\")\n",
    "print(f\"Traditional  : {step_times['Step 3']:.2f}s\")\n",
    "print(f\"Speedup      : {step_times['Step 3'] / spark_times['Word Frequency']:.2f}x\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c21edfb8-422d-4409-ad8c-8818e79d24d4",
   "metadata": {},
   "source": [
    "### MapReduce Optimisation 2: Metadata Join\n",
    "\n",
    "**Map (reviews):** Emit `(asin, (rating, 1))` for each review record.\n",
    "\n",
    "**Map (metadata):** Emit `(asin, (title, category, price))` for each product.\n",
    "\n",
    "**Reduce:** Aggregate review stats, then join with metadata by `asin` key — distributed hash join.\n",
    "\n",
    "Expected speedup: ~3-4x over the 92s baseline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a7313197-f60e-4b26-a1fb-35a586f70a98",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MapReduce Step 2: Metadata join...\n",
      "Joined products : 177,724\n",
      "Spark time      : 58.83s\n",
      "Traditional     : 104.62s\n",
      "Speedup         : 1.78x\n"
     ]
    }
   ],
   "source": [
    "print(\"MapReduce Step 2: Metadata join...\")\n",
    "\n",
    "# Convert already-loaded meta dict to a list for parallelisation\n",
    "meta_list = [(asin, (info[\"title\"], info[\"category\"], info[\"price\"]))\n",
    "             for asin, info in meta.items()]\n",
    "\n",
    "def map_review_for_join(review_dict):\n",
    "    # Map: emit (asin, (rating, 1))\n",
    "    return (review_dict[\"asin\"], (review_dict[\"rating\"], 1))\n",
    "\n",
    "start = time.time()\n",
    "\n",
    "# Reduce reviews: aggregate to (asin, avg_rating)\n",
    "review_stats_rdd = sc.parallelize(reviews, sc.defaultParallelism) \\\n",
    "    .map(map_review_for_join) \\\n",
    "    .reduceByKey(lambda a, b: (a[0] + b[0], a[1] + b[1])) \\\n",
    "    .mapValues(lambda x: round(x[0] / x[1], 2))\n",
    "\n",
    "# Parallelise metadata\n",
    "meta_rdd = sc.parallelize(meta_list, sc.defaultParallelism)\n",
    "\n",
    "# Join by asin key\n",
    "joined_rdd = review_stats_rdd.join(meta_rdd)\n",
    "result_count = joined_rdd.count()\n",
    "\n",
    "spark_times[\"Metadata Join\"] = time.time() - start\n",
    "\n",
    "print(f\"Joined products : {result_count:,}\")\n",
    "print(f\"Spark time      : {spark_times['Metadata Join']:.2f}s\")\n",
    "print(f\"Traditional     : {step_times['Step 4']:.2f}s\")\n",
    "print(f\"Speedup         : {step_times['Step 4'] / spark_times['Metadata Join']:.2f}x\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7b4951a-2e76-4590-8ed3-1eea28cfc4f1",
   "metadata": {},
   "source": [
    "### Final Performance Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "2d870a99-94a1-4b68-8c0d-33f10117d424",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==========================================================\n",
      "Performance Comparison: Traditional vs Spark RDD\n",
      "==========================================================\n",
      "Task                       Traditional  Spark RDD  Speedup\n",
      "----------------------------------------------------------\n",
      "Word Frequency (Step 3)         35.41s     36.32s    0.97x\n",
      "Metadata Join (Step 4)         104.62s     58.83s    1.78x\n",
      "==========================================================\n"
     ]
    }
   ],
   "source": [
    "print(\"=\" * 58)\n",
    "print(\"Performance Comparison: Traditional vs Spark RDD\")\n",
    "print(\"=\" * 58)\n",
    "print(f\"{'Task':<25} {'Traditional':>12} {'Spark RDD':>10} {'Speedup':>8}\")\n",
    "print(\"-\" * 58)\n",
    "print(f\"{'Word Frequency (Step 3)':<25} {step_times['Step 3']:>11.2f}s \"\n",
    "      f\"{spark_times['Word Frequency']:>9.2f}s \"\n",
    "      f\"{step_times['Step 3']/spark_times['Word Frequency']:>7.2f}x\")\n",
    "print(f\"{'Metadata Join (Step 4)':<25} {step_times['Step 4']:>11.2f}s \"\n",
    "      f\"{spark_times['Metadata Join']:>9.2f}s \"\n",
    "      f\"{step_times['Step 4']/spark_times['Metadata Join']:>7.2f}x\")\n",
    "print(\"=\" * 58)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a53e98d-66e2-4d31-a78b-21bbf0ed755f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
